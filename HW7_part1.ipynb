{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF552 HW7 Zhifeng Liu 2912549136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generative Models for Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C i. Concatenate your text files to create a corpus of Russell’s writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('/Users/zhifengliu/Desktop/552 HW7') if '.txt' in f]\n",
    "with open('combine.txt', 'w') as outfile:\n",
    "    for file in files:\n",
    "        with open(file) as infile:\n",
    "            content = infile.read().replace('\\n', ' ')\n",
    "            outfile.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C ii. Use a character-level representation for this model by using extended ASCII that has N = 256 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245121\n"
     ]
    }
   ],
   "source": [
    "input_file = 'combine.txt'\n",
    "text = ''\n",
    "with open(input_file, 'r') as file:\n",
    "    text = file.read()\n",
    "print(len(text))\n",
    "text_asc = [ord(c) for c in text if ord(c) < 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C iii. iv. Choose a window size, e.g.,\n",
    "Inputs to the network will be the first W −1 = 99 characters of each sequence, and the output of the network will be the Wth character of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=100\n",
    "train_x = []\n",
    "train_y = []\n",
    "for i in range(0, len(text_asc)-w, 1):\n",
    "    x = text_asc[i:i+w-1]\n",
    "    y = text_asc[i+w-1]\n",
    "    train_x.append(x)\n",
    "    train_y.append(y)\n",
    "train_x = np.reshape(train_x, (len(train_x), w-1, 1))\n",
    "train_x = train_x/float(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C v. using a one-hot encoding scheme to enconde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "encode = OneHotEncoder(n_values=256)\n",
    "train_y = np.reshape(train_y,(len(train_y), 1))\n",
    "encode.fit(train_y)\n",
    "one_hot_y = encode.transform(train_y).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C vi. vii. Use a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "Use a Softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "model.add(Dense(256, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath='30epochs.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C ix. x. Train 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.9922\n",
      "Epoch 00001: loss improved from inf to 2.99225, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1126s 5ms/sample - loss: 2.9923\n",
      "Epoch 2/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.8360\n",
      "Epoch 00002: loss improved from 2.99225 to 2.83595, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1164s 5ms/sample - loss: 2.8360\n",
      "Epoch 3/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.7758\n",
      "Epoch 00003: loss improved from 2.83595 to 2.77575, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1164s 5ms/sample - loss: 2.7758\n",
      "Epoch 4/30\n",
      "244992/245021 [============================>.] - ETA: 3s - loss: 2.7380 \n",
      "Epoch 00004: loss improved from 2.77575 to 2.73798, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 25806s 105ms/sample - loss: 2.7380\n",
      "Epoch 5/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.7029\n",
      "Epoch 00005: loss improved from 2.73798 to 2.70286, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1183s 5ms/sample - loss: 2.7029\n",
      "Epoch 6/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.6750\n",
      "Epoch 00006: loss improved from 2.70286 to 2.67498, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1202s 5ms/sample - loss: 2.6750\n",
      "Epoch 7/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.6508\n",
      "Epoch 00007: loss improved from 2.67498 to 2.65079, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1183s 5ms/sample - loss: 2.6508\n",
      "Epoch 8/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.6346\n",
      "Epoch 00008: loss improved from 2.65079 to 2.63462, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1149s 5ms/sample - loss: 2.6346\n",
      "Epoch 9/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.6120\n",
      "Epoch 00009: loss improved from 2.63462 to 2.61200, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1126s 5ms/sample - loss: 2.6120\n",
      "Epoch 10/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5899\n",
      "Epoch 00010: loss improved from 2.61200 to 2.58995, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1120s 5ms/sample - loss: 2.5899\n",
      "Epoch 11/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5687\n",
      "Epoch 00011: loss improved from 2.58995 to 2.56867, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1153s 5ms/sample - loss: 2.5687\n",
      "Epoch 12/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5470\n",
      "Epoch 00012: loss improved from 2.56867 to 2.54704, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1171s 5ms/sample - loss: 2.5470\n",
      "Epoch 13/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5279\n",
      "Epoch 00013: loss improved from 2.54704 to 2.52788, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1204s 5ms/sample - loss: 2.5279\n",
      "Epoch 14/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5142\n",
      "Epoch 00014: loss improved from 2.52788 to 2.51417, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1181s 5ms/sample - loss: 2.5142\n",
      "Epoch 15/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4913\n",
      "Epoch 00015: loss improved from 2.51417 to 2.49134, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1190s 5ms/sample - loss: 2.4913\n",
      "Epoch 16/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4756\n",
      "Epoch 00016: loss improved from 2.49134 to 2.47560, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1191s 5ms/sample - loss: 2.4756\n",
      "Epoch 17/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4593\n",
      "Epoch 00017: loss improved from 2.47560 to 2.45927, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1199s 5ms/sample - loss: 2.4593\n",
      "Epoch 18/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5211\n",
      "Epoch 00018: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1155s 5ms/sample - loss: 2.5211\n",
      "Epoch 19/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.7211\n",
      "Epoch 00019: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1102s 4ms/sample - loss: 2.7211\n",
      "Epoch 20/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.6402\n",
      "Epoch 00020: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1108s 5ms/sample - loss: 2.6402\n",
      "Epoch 21/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5931\n",
      "Epoch 00021: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1098s 4ms/sample - loss: 2.5931\n",
      "Epoch 22/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5539\n",
      "Epoch 00022: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1103s 5ms/sample - loss: 2.5539\n",
      "Epoch 23/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5247\n",
      "Epoch 00023: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1104s 5ms/sample - loss: 2.5247\n",
      "Epoch 24/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.5014\n",
      "Epoch 00024: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1105s 5ms/sample - loss: 2.5014\n",
      "Epoch 25/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4812\n",
      "Epoch 00025: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1109s 5ms/sample - loss: 2.4812\n",
      "Epoch 26/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4670\n",
      "Epoch 00026: loss did not improve from 2.45927\n",
      "245021/245021 [==============================] - 1109s 5ms/sample - loss: 2.4670\n",
      "Epoch 27/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4453\n",
      "Epoch 00027: loss improved from 2.45927 to 2.44528, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1105s 5ms/sample - loss: 2.4453\n",
      "Epoch 28/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4330\n",
      "Epoch 00028: loss improved from 2.44528 to 2.43300, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1107s 5ms/sample - loss: 2.4330\n",
      "Epoch 29/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.4163\n",
      "Epoch 00029: loss improved from 2.43300 to 2.41634, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1111s 5ms/sample - loss: 2.4163\n",
      "Epoch 30/30\n",
      "244992/245021 [============================>.] - ETA: 0s - loss: 2.3994\n",
      "Epoch 00030: loss improved from 2.41634 to 2.39939, saving model to 30epochs.hdf5\n",
      "245021/245021 [==============================] - 1116s 5ms/sample - loss: 2.3994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2b032710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, one_hot_y, epochs=30, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C xi. Generate 1000 characters text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(init_text, length):\n",
    "    text_asc_array = [ord(c) for c in init_text if ord(c) < 256]\n",
    "    text_asc_array = np.array(text_asc_array)\n",
    "    text_asc_array = text_asc_array/float(256)\n",
    "    input_x = text_asc_array[-99:]\n",
    "    input_x = input_x.tolist()\n",
    "    output_text = init_text\n",
    "    for i in range(length):\n",
    "        pred_x = np.reshape(input_x, (1, len(train_x[0]), 1))\n",
    "        predict = model.predict(pred_x, verbose=0)\n",
    "        index = np.argmax(predict)\n",
    "        new_char = chr(index)\n",
    "        new_x = ord(new_char)/float(256)\n",
    "        input_x.append(new_x)\n",
    "        input_x = input_x[1:]\n",
    "        output_text += new_char\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. ahd the sane th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th the sene th'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_text = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'\n",
    "generate_text(init_text, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
